{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7ZNUN08c7uO"
      },
      "source": [
        "# Theoretical Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sw7e068Sc7uQ"
      },
      "source": [
        "* This is the theoretical part of the final project. It includes theoretical questions from various topics covered in the course.\n",
        "* There are 7 questions among which you need to choose 6, according to the following key:\n",
        "    + Question 1 is **mandatory**.\n",
        "    + Choose **one question** from questions 2-3.\n",
        "    + Question 4 is **mandatory**.\n",
        "    + Questions 5-6 are **mandatory**.\n",
        "    + Question 7 is **mandatory**.\n",
        "* Question 1 is worth 15 points, whereas the other questions worth 7 points.\n",
        "* All in all, the maximal grade for this parts is 15+7*5=50 points.\n",
        "* **You should answer the questions on your own. We will check for plagiarism.**\n",
        "* If you need to add external images (such as graphs) to this notebook, please put them inside the 'imgs' folder. DO NOT put a reference to an external link.\n",
        "* Good luck!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9B5GsL1c7uR"
      },
      "source": [
        "## Part 1: General understanding of the course material"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yEhVJrmc7uR"
      },
      "source": [
        "### Question 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwDLigT4c7uR"
      },
      "source": [
        "1.  Relate the number of parameters in a neural network to the over-fitting phenomenon (*).\n",
        "    Relate this to the design of convolutional neural networks, and explain why CNNs are a plausible choice for an hypothesis class for visual classification tasks.\n",
        "\n",
        "    (*) In the context of classical under-fitting/over-fitting in machine learning models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwapgMKyc7uS"
      },
      "source": [
        "#### Answer:\n",
        "More parameters in a neural network means that the learned metwork can be more complex and sueted to the training data we used in the training procces. <br>\n",
        "Complex model is much more prone to overfitting, since it can \"remember\" more specific relations from the training set thus overfitted to it. <br>\n",
        "If we consider the number of parameters of convolutional neural networks, this number depends on the number of layers (might be activation, pooling,  <br>\n",
        "fully connected, convulotion etc.), channels in each layer, kernel, stride and padding sizes. So increasing any of those parameter may lead to overfitting. <br>\n",
        "From the other hand, CNN tries to solve the overfitting phenomenon by reducing the number of parameters (versus FC for example) by using weighs in size of <br>\n",
        "kernel_size*kernel_size only instead of the entire size of the image as weighs. Another things is the fact the CNN let us detect pattern in given image by  <br>\n",
        "appling the kernel on the entire image, thus, finding pattern in the image regardless of its position in the images.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBgTEsa3c7uS"
      },
      "source": [
        "2. Consider the linear classifier model with hand-crafted features:\n",
        "    $$f_{w,b}(x) = w^T \\psi(x) + b$$\n",
        "    where $x \\in \\mathbb{R}^2$, $\\psi$ is a non-learnable feature extractor and assume that the classification is done by $sign(f_{w,b}(x))$. Let $\\psi$ be the following feature extractor $\\psi(x)=x^TQx$ where $Q \\in \\mathbb{R}^{2 \\times 2}$ is a non-learnable positive definite matrix. Describe a distribution of the data which the model is able to approximate, but the simple linear model fails to approximate (hint: first, try to describe the decision boundary of the above classifier)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUuRVoVsc7uT"
      },
      "source": [
        "#### Answer:\n",
        "The decision boundry is achived when $$f_{w,b}(x) = 0$$ because the classification is done by $sign(f_{w,b}(x))$.\n",
        "So:  $$ f_{w,b}(x) = w^T \\psi(x) + b = w^T x^TQx + b = 0$$ <br>\n",
        "Which is equation from 2nd degree, meaninig the decision boundry is an hyperplane of 2nd degree.\n",
        "Thus, the distribution of the data which the model is able to approximate are linear (included in the space of 2nd degrees field) too,\n",
        "but it also can approx. data that it's features have up to 2nd degree relations between them.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FmoThEoc7uT"
      },
      "source": [
        "3. Assume that we would like to train a Neural Network for classifying images into $C$ classes. Assume that the architecture can be stored in the memory as a computational graph with $N$ nodes where the output is the logits (namely, before applying softmax) for the current batch ($f_w: B \\times Ch \\times H \\times W \\rightarrow B \\times C$). Assume that the computational graph operates on *tensor* values.\n",
        "    * Implement the CE loss assuming that the labels $y$ are hard labels given in a LongTensor (as usual). **Use Torch's log_softmax and index_select functions** and implement with less as possible operations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8bqyiHic7uU"
      },
      "source": [
        "#### Answer:\n",
        "$$ CE_LOSS(x, y) =  -1 \\cdot sum(y \\cdot log(model(x))) = -1 \\cdot sum(y \\cdot y_probs) $$ <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imzBRqbac7uU",
        "outputId": "387ac1b3-e313-4888-a65c-60871b8e3f45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-4d2dd52c80fe>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Output: the loss on the current batch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0my_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "from torch.nn.functional import log_softmax\n",
        "from torch import index_select\n",
        "import torch\n",
        "\n",
        "# Input:  model, x, y.\n",
        "# Output: the loss on the current batch.\n",
        "\n",
        "logits = model(x)\n",
        "y_probs = log_softmax(input=logits, dim=1)\n",
        "loss = -1 * torch.sum(index_select(input=y_probs, dim=1, incices=y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pl-yQFulc7uV"
      },
      "source": [
        "* Using the model's function as a black box, draw the computational graph (treating both log_softmax and index_select as an atomic operations). How many nodes are there in the computational graph?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DR9igofBc7uW"
      },
      "source": [
        "#### Answer:\n",
        "There will be 7 nodes:\n",
        "<center><img src=\"imgs/ce_loss.png\" /></center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqpC0Yw0c7uW"
      },
      "source": [
        "* Now, instead of using hard labels, assume that the labels are representing some probability distribution over the $C$ classes. How would the gradient computation be affected? analyze the growth in the computational graph, memory and computation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OHxzOfbc7uW"
      },
      "source": [
        "#### Answer:\n",
        "Now the computational graph would look like this:\n",
        "<center><img src=\"imgs/ce_loss2.png\" /></center>\n",
        "\n",
        "Meaning we will must add another step to our gradient computation - the dot product between y and y_log_prob, as now it isn't one-hot and it means we must take into account the entire probabilties in the calc (when it was one-hot we could look only at one probability - 1).\n",
        "It means that both the memory and computation usage will be increased.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_YZPd8Oc7uW"
      },
      "source": [
        "* Apply the same analysis in the case that we would like to double the batch size. How should we change the learning rate of the optimizer?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDvoUZgOc7uW"
      },
      "source": [
        "#### Answer:\n",
        "In case we would double the batches size, the gradient computation would stay (pretty much) the same but the gradient itself will be more accurate since we consider more samples in the batches. Also we would use more GPU mem at a given time, since we would like to store the data from the current batch. The computational graph will stay the same size but the operation will change (e.g. from sum of 16 values it would increase to sum of 32 values). Finally, the learning rate must be decreased in order to maintain roughly the same sizes of steps as before, since we aggragate the gradients of the batches (which we increased the size of them)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFLTmHNwc7uW"
      },
      "source": [
        "## Part 2: Optimization & Automatic Differentiation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wg6cMrYEc7uZ"
      },
      "source": [
        "### Question 2: resolving gradient conflicts in multi-task learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoDj_RKoc7uZ"
      },
      "source": [
        "Assume that you want to train a model to perform two tasks: task 1 and task 2.\n",
        "For each such task $i$ you have an already implemented function *loss\\_i = forward_and_compute_loss_i(model,inputs)* such that given the model and the inputs it computes the loss w.r.t task $i$ (assume that the computational graph is properly constructed). We would like to train our model using SGD to succeed in both tasks as follows: in each training iteration (batch) -\n",
        "* Let $g_i$ be the gradient w.r.t the $i$-th task.\n",
        "* If $g_1 \\cdot g_2 < 0$:\n",
        "    + Pick a task $i$ at random.\n",
        "    + Apply GD w.r.t only that task.\n",
        "* Otherwise:\n",
        "    + Apply GD w.r.t both tasks (namely $\\mathcal{L}_1 + \\mathcal{L}_2$).\n",
        "\n",
        "Note that in the above formulation the gradient is a thought of as a concatination of all the gradient w.r.t all the models parameters, and $g_1 \\cdot g_2$ stands for a dot product.\n",
        "\n",
        "What parts should be modified to implement the above? Is it the optimizer, the training loop or both? Implement the above algorithm in a code cell/s below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5W5n1U6_c7ua"
      },
      "source": [
        "### Question 3: manual automatic differentiation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMDJ7jyCc7ua"
      },
      "source": [
        "Consider the following two-input two-output function:\n",
        "$$ f(x,y) = (x^2\\sin(xy+\\frac{\\pi}{2}), x^2\\ln(1+xy)) $$\n",
        "* Draw a computational graph for the above function. Assume that the unary atomic units are squaring, taking square root, $\\exp,\\ln$, basic trigonometric functions and the binary atomic units are addition and multiplication. You would have to use constant nodes.\n",
        "* Calculate manually the forward pass.\n",
        "* Calculate manually the derivative of all outputs w.r.t all inputs using a forward mode AD.\n",
        "* Calculate manually the derivative of all outputs w.r.t all inputs using a backward mode AD."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyaip3h2c7ua"
      },
      "source": [
        "## Part 3: Sequential Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfkvnF2Nc7ua"
      },
      "source": [
        "### Question 4: RNNs vs Transformers in the real life"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M26m8dpsc7ua"
      },
      "source": [
        "In each one of the following scenarios decide whether to use RNN based model or a transformer based model. Justify your choice.\n",
        "1. You are running a start-up in the area of automatic summarization of academic papers. The inference of the model is done on the server side, and it is very important for it to be fast.\n",
        "2. You need to design a mobile application that gathers small amount of data from few apps in every second and then uses a NN to possibly generate an alert given the information in the current second and the information from the past minute.\n",
        "3. You have a prediction task over fixed length sequences on which you know the following properties:\n",
        "    + In each sequence there are only few tokens that the model should attend to.\n",
        "    + Most of the information needed for generating a reliable prediction is located at the beginning of the sequence.\n",
        "    + There is no restriction on the computational resources."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Answer\n",
        "\n",
        "1:\n",
        "\n",
        "In this scenario, your startup focuses on rapidly summarizing academic papers on the server side. Speed is of utmost importance. Therefore, opting for a Transformer-based model is advantageous. Transformers are known for their parallel processing capabilities, which significantly accelerate inference. Modern transformer architectures, such as BERT and GPT, have demonstrated remarkable performance in natural language understanding and generation tasks. This aligns perfectly with the task of summarizing academic papers, as it ensures both the speed and quality of the generated summaries.\n",
        "\n",
        "2:\n",
        "\n",
        "For the development of a mobile application that continuously collects small data snippets from various apps every second and requires real-time processing with a focus on temporal dependencies, choosing an RNN-based model is prudent. RNNs excel in handling sequential data and capturing patterns over time. They are well-suited for scenarios where past information significantly influences the current state, as is the case with real-time data gathering. RNNs can efficiently process streaming data and make predictions that rely on the temporal relationships within the data, making them a fitting choice for this mobile application.\n",
        "\n",
        "3:\n",
        "\n",
        "In this scenario, the prediction task involves fixed-length sequences with a specific focus on only a few tokens, where most of the essential information is concentrated at the beginning of each sequence, and there are no computational constraints. Here, opting for a Transformer-based model is likely the more advantageous choice. Transformers are exceptionally versatile and capable of attending to specific positions within a sequence, even when the critical information is located at the beginning. They can efficiently capture long-range dependencies and intricate relationships in the data, potentially leading to superior predictive performance. Given the abundance of computational resources, configuring a transformer to precisely focus on the relevant tokens and extract valuable insights from the data becomes feasible."
      ],
      "metadata": {
        "id": "clmk5G5AV-w2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0o2RrTjc7ub"
      },
      "source": [
        "## Part 4: Generative modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxHbGiRnc7ub"
      },
      "source": [
        "### Question 5: VAEs and GANS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6O4f83wmc7ub"
      },
      "source": [
        "Suggest a method for combining VAEs and GANs. Focus on the different components of the model and how to train them jointly (the objectives). Which drawbacks of these models the combined model may overcome? which not?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Answer\n",
        "Here’s a suggested method for combining VAEs and GANs:\n",
        "\n",
        "Model Components:\n",
        "Encoder (Q-network in VAE terminology):\n",
        "\n",
        "Input: Data sample.\n",
        "Output: Mean and variance of the latent space distribution.\n",
        "Decoder (P-network in VAE terminology, or the Generator in GAN terminology):\n",
        "\n",
        "Input: Sample from the latent space.\n",
        "Output: Data sample.\n",
        "Discriminator (from the GAN):\n",
        "\n",
        "Input: Real or generated data sample.\n",
        "Output: Probability that the input is a real sample.\n",
        "Joint Training:\n",
        "VAE Objective:\n",
        "\n",
        "The VAE’s objective can be split into two terms:\n",
        "Reconstruction loss (often Mean Squared Error between original and reconstructed image).\n",
        "KL divergence between the encoder's output distribution and a prior (usually a standard normal distribution).\n",
        "Together, these ensure that the VAE produces meaningful latent representations and can reconstruct data samples from these representations.\n",
        "GAN Objective:\n",
        "\n",
        "For the Generator (Decoder): Minimize the difference between the Discriminator's output for the generated samples and an array of ones (trying to fool the Discriminator).\n",
        "For the Discriminator: Maximize the difference between its output for real samples and an array of ones and its output for generated samples and an array of zeros.\n",
        "Combined Training:\n",
        "\n",
        "During training, the VAE's objectives and the GAN's objectives are combined. One common approach is to alternate between updating the VAE's weights (both encoder and decoder) and the GAN's weights (both generator and discriminator).\n",
        "Benefits:\n",
        "Stable Training: VAEs generally have more stable training than GANs due to the explicit likelihood-based objective.\n",
        "Better Reconstructions: VAEs can often produce more accurate reconstructions of data, while GANs can sometimes miss certain modes. The combined model might produce sharper and more accurate reconstructions than a standalone VAE.\n",
        "Regularized Latent Space: The VAE component ensures that the latent space has good continuity and coverage.\n",
        "Drawbacks:\n",
        "Increased Complexity: Combining the architectures increases the model's complexity, which can make it harder to train and tune.\n",
        "Mode Dropping: While the combination might alleviate some of the mode dropping seen in GANs, it might not completely eliminate it.\n",
        "Training Stability: Even though the VAE component can add some stability, GANs are notorious for training instability, and some of those challenges may persist.\n",
        "In summary, while VAE-GANs combine the benefits of both architectures, they also combine some of their challenges. However, the strengths of one can sometimes help mitigate the weaknesses of the other. Experimentation and careful tuning are crucial when working with such combined models.\n",
        "\n",
        "-----------------------------------------\n",
        "\n",
        "Certainly! When combining VAEs and GANs, the alternating training scheme will involve updating the parameters based on the VAE loss and the GAN loss in an alternating fashion. Below is a more structured approach in LaTeX.\n",
        "\n",
        "Define the VAE Losses:\n",
        "For the VAE, the objective typically includes both the reconstruction loss and the KL-divergence loss.\n",
        "�\n",
        "recon\n",
        "=\n",
        "∣\n",
        "∣\n",
        "�\n",
        "−\n",
        "Decoder\n",
        "(\n",
        "�\n",
        ")\n",
        "∣\n",
        "∣\n",
        "2\n",
        "L\n",
        "recon\n",
        "​\n",
        " =∣∣x−Decoder(z)∣∣\n",
        "2\n",
        "\n",
        "�\n",
        "KL\n",
        "=\n",
        "−\n",
        "1\n",
        "2\n",
        "∑\n",
        "�\n",
        "=\n",
        "1\n",
        "�\n",
        "(\n",
        "1\n",
        "+\n",
        "log\n",
        "⁡\n",
        "(\n",
        "(\n",
        "�\n",
        "�\n",
        ")\n",
        "2\n",
        ")\n",
        "−\n",
        "(\n",
        "�\n",
        "�\n",
        ")\n",
        "2\n",
        "−\n",
        "(\n",
        "�\n",
        "�\n",
        ")\n",
        "2\n",
        ")\n",
        "L\n",
        "KL\n",
        "​\n",
        " =−\n",
        "2\n",
        "1\n",
        "​\n",
        "  \n",
        "i=1\n",
        "∑\n",
        "D\n",
        "​\n",
        " (1+log((σ\n",
        "i\n",
        "​\n",
        " )\n",
        "2\n",
        " )−(μ\n",
        "i\n",
        "​\n",
        " )\n",
        "2\n",
        " −(σ\n",
        "i\n",
        "​\n",
        " )\n",
        "2\n",
        " )\n",
        "Combined VAE Loss:\n",
        "\n",
        "�\n",
        "VAE\n",
        "=\n",
        "�\n",
        "recon\n",
        "+\n",
        "�\n",
        "KL\n",
        "�\n",
        "KL\n",
        "L\n",
        "VAE\n",
        "​\n",
        " =L\n",
        "recon\n",
        "​\n",
        " +λ\n",
        "KL\n",
        "​\n",
        " L\n",
        "KL\n",
        "​\n",
        "\n",
        "Define the GAN Losses:\n",
        "For the Discriminator:\n",
        "�\n",
        "D-real\n",
        "=\n",
        "−\n",
        "log\n",
        "⁡\n",
        "(\n",
        "Discriminator\n",
        "(\n",
        "�\n",
        ")\n",
        ")\n",
        "L\n",
        "D-real\n",
        "​\n",
        " =−log(Discriminator(x))\n",
        "�\n",
        "D-fake\n",
        "=\n",
        "−\n",
        "log\n",
        "⁡\n",
        "(\n",
        "1\n",
        "−\n",
        "Discriminator\n",
        "(\n",
        "Decoder\n",
        "(\n",
        "�\n",
        ")\n",
        ")\n",
        ")\n",
        "L\n",
        "D-fake\n",
        "​\n",
        " =−log(1−Discriminator(Decoder(z)))\n",
        "Combined Discriminator Loss:\n",
        "\n",
        "�\n",
        "D\n",
        "=\n",
        "�\n",
        "D-real\n",
        "+\n",
        "�\n",
        "D-fake\n",
        "L\n",
        "D\n",
        "​\n",
        " =L\n",
        "D-real\n",
        "​\n",
        " +L\n",
        "D-fake\n",
        "​\n",
        "\n",
        "For the Generator (Decoder in VAE context):\n",
        "\n",
        "�\n",
        "G\n",
        "=\n",
        "−\n",
        "log\n",
        "⁡\n",
        "(\n",
        "Discriminator\n",
        "(\n",
        "Decoder\n",
        "(\n",
        "�\n",
        ")\n",
        ")\n",
        ")\n",
        "L\n",
        "G\n",
        "​\n",
        " =−log(Discriminator(Decoder(z)))\n",
        "Alternating Training Procedure in LaTeX:\n",
        "while not converged:\n",
        "while not converged:\n",
        "1. Sample a batch of data,\n",
        "�\n",
        " from the training dataset\n",
        "1. Sample a batch of data, x from the training dataset\n",
        "2. Forward pass through the VAE:\n",
        "2. Forward pass through the VAE:\n",
        "a. Compute\n",
        "�\n",
        ",\n",
        "�\n",
        ",\n",
        " and\n",
        "�\n",
        " using the Encoder\n",
        "a. Compute z,μ, and σ using the Encoder\n",
        "b. Compute\n",
        "�\n",
        "recon\n",
        ",\n",
        "�\n",
        "KL\n",
        ",\n",
        " and\n",
        "�\n",
        "VAE\n",
        "b. Compute L\n",
        "recon\n",
        "​\n",
        " ,L\n",
        "KL\n",
        "​\n",
        " , and L\n",
        "VAE\n",
        "​\n",
        "\n",
        "c. Backpropagate\n",
        "�\n",
        "VAE\n",
        " and update Encoder and Decoder parameters\n",
        "c. Backpropagate L\n",
        "VAE\n",
        "​\n",
        "  and update Encoder and Decoder parameters\n",
        "3. For GAN training:\n",
        "3. For GAN training:\n",
        "a. Sample random latent vectors and generate fake data using Decoder\n",
        "a. Sample random latent vectors and generate fake data using Decoder\n",
        "b. Compute\n",
        "�\n",
        "D-real\n",
        ",\n",
        "�\n",
        "D-fake\n",
        ",\n",
        " and\n",
        "�\n",
        "D\n",
        "b. Compute L\n",
        "D-real\n",
        "​\n",
        " ,L\n",
        "D-fake\n",
        "​\n",
        " , and L\n",
        "D\n",
        "​\n",
        "\n",
        "c. Backpropagate\n",
        "�\n",
        "D\n",
        " and update Discriminator parameters\n",
        "c. Backpropagate L\n",
        "D\n",
        "​\n",
        "  and update Discriminator parameters\n",
        "d. Compute\n",
        "�\n",
        "G\n",
        "d. Compute L\n",
        "G\n",
        "​\n",
        "\n",
        "e. Backpropagate\n",
        "�\n",
        "G\n",
        " and update Decoder (Generator) parameters\n",
        "e. Backpropagate L\n",
        "G\n",
        "​\n",
        "  and update Decoder (Generator) parameters\n",
        "In this approach, during each iteration of training, you're first updating the Encoder and Decoder using the VAE objective, and then updating the Discriminator and Decoder using the GAN objective.\n"
      ],
      "metadata": {
        "id": "55riTgXzU41Y"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sy0_9cUFc7ub"
      },
      "source": [
        "### Question 6: Diffusion Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhDViH6Ec7ub"
      },
      "source": [
        "Show that $q(x_{t-1}|x_t,x_0)$ is tractable and is given by $\\mathcal{N}(x_{t-1};\\tilde{\\mu}(x_t,x_0),\\tilde{\\beta_t}I)$ where the terms for $\\tilde{\\mu}(x_t,x_0)$ and $\\tilde{\\beta_t}$ are given in the last tutorial. Do so by explicitly computing the PDF."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwU3YD4Uc7ub"
      },
      "source": [
        "## Part 5: Training Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3tjxONHc7ub"
      },
      "source": [
        "### Question 7: Batch Normalization and Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTli4KZ3c7ub"
      },
      "source": [
        "For both BatchNorm and Dropout analyze the following:\n",
        "1. How to use them during the training phase (both in forward pass and backward pass)?\n",
        "2. How differently they behave in the inference phase? How to distinguish these operation modes in code?\n",
        "3. Assume you would like to perform multi-GPU training (*) to train your model. What should be done in order for BatchNorm and dropout to work properly? assume that each process holds its own copy of the model and that the processes can share information with each other.\n",
        "\n",
        "(*): In a multi-GPU training each GPU is associated with its own process that holds an independent copy of the model. In each training iteration a (large) batch is split among these processes (GPUs) which compute the gradients of the loss w.r.t the relevant split of the data. Afterwards, the gradients from each process are then shared and averaged so that the GD would take into account the correct gradient and to assure synchornization of the model copies. Note that the proccesses are blocked between training iterations."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Answer\n",
        "\n",
        "Let's break down how Batch Normalization (BatchNorm) and Dropout work during the training phase, how they behave differently during inference, and what considerations are necessary for multi-GPU training.\n",
        "\n",
        "**Batch Normalization (BatchNorm):**\n",
        "\n",
        "1. **Training Phase (Forward Pass):**\n",
        "   - During the forward pass in training, BatchNorm normalizes the activations of each layer in a mini-batch. This involves subtracting the mean and dividing by the standard deviation for each feature channel.\n",
        "   - It also maintains moving averages of the mean and variance for each channel to be used during inference.\n",
        "   - BatchNorm then scales and shifts the normalized values using learnable parameters (gamma and beta).\n",
        "   - The normalized activations are passed through the activation function, and the result is used for further computation in the network.\n",
        "   - Batch statistics (mean and variance) are computed for the current mini-batch and used for normalization.\n",
        "\n",
        "2. **Training Phase (Backward Pass):**\n",
        "   - During backpropagation, gradients are computed with respect to the normalized activations, gamma, and beta.\n",
        "   - These gradients are used to update the model's parameters during optimization.\n",
        "\n",
        "3. **Inference Phase:**\n",
        "   - In the inference phase, BatchNorm uses the previously calculated moving averages of mean and variance to normalize the activations.\n",
        "   - Gamma and beta, which were learned during training, are applied to scale and shift the normalized values.\n",
        "   - There is no need for computing batch statistics during inference.\n",
        "\n",
        "**Dropout:**\n",
        "\n",
        "1. **Training Phase (Forward Pass):**\n",
        "   - During the forward pass in training, Dropout randomly sets a fraction of the activations to zero. This helps prevent overfitting.\n",
        "   - The probability of dropping out a neuron is a hyperparameter called the dropout rate.\n",
        "\n",
        "2. **Training Phase (Backward Pass):**\n",
        "   - During backpropagation, gradients are computed as if the dropped-out neurons were still active. This means no adjustments are made to the gradients due to dropout.\n",
        "\n",
        "3. **Inference Phase:**\n",
        "   - In the inference phase, dropout is typically turned off. All neurons are active.\n",
        "   - However, it's important to note that the weights may need to be scaled during inference. This scaling is usually done by multiplying the weights by the dropout rate.\n",
        "\n",
        "**Multi-GPU Training:**\n",
        "\n",
        "In multi-GPU training, where each GPU has an independent copy of the model, several considerations are needed for BatchNorm and Dropout to work properly:\n",
        "\n",
        "1. **Batch Normalization:**\n",
        "   - Each GPU should compute its batch statistics independently during the forward pass.\n",
        "   - After the forward pass on each GPU, the statistics (mean and variance) should be averaged across all GPUs to ensure consistent normalization.\n",
        "   - These synchronized statistics should then be used for the normalization on each GPU during the forward and backward passes.\n",
        "   - The gamma and beta parameters should also be synchronized or shared among all GPUs.\n",
        "\n",
        "2. **Dropout:**\n",
        "   - When using dropout in multi-GPU training, ensure that the same dropout mask is applied on all GPUs to maintain consistency.\n",
        "   - If the dropout mask is generated randomly, you can use a shared random seed or synchronize the masks among GPUs to achieve this consistency.\n",
        "\n",
        "To distinguish between training and inference modes in code, most deep learning frameworks provide a flag or mode setting. For example, in PyTorch, you can use `model.train()` to set the model in training mode and `model.eval()` to set it in evaluation (inference) mode. This affects the behavior of BatchNorm and Dropout layers as described above."
      ],
      "metadata": {
        "id": "5qoxh-6lYxY4"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}